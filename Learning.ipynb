{"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNH9w354GZ9bCavvqYD2hvn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install pyspark","metadata":{"id":"qO29vGOHS8Q8","outputId":"1c13749c-a655-4904-8c77-36b748d4847a","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2022-10-13T02:08:46.883929Z","iopub.execute_input":"2022-10-13T02:08:46.884676Z","iopub.status.idle":"2022-10-13T02:09:40.759633Z","shell.execute_reply.started":"2022-10-13T02:08:46.884633Z","shell.execute_reply":"2022-10-13T02:09:40.758024Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pyspark\n  Downloading pyspark-3.3.0.tar.gz (281.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.3/281.3 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting py4j==0.10.9.5\n  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.0-py2.py3-none-any.whl size=281764026 sha256=0fe27e6d3dcacb137b708b09d1e3f3df24befbfdc85473d742f5e1c8b1ca6c47\n  Stored in directory: /root/.cache/pip/wheels/7a/8e/1b/f73a52650d2e5f337708d9f6a1750d451a7349a867f928b885\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\n  Attempting uninstall: py4j\n    Found existing installation: py4j 0.10.9.7\n    Uninstalling py4j-0.10.9.7:\n      Successfully uninstalled py4j-0.10.9.7\nSuccessfully installed py4j-0.10.9.5 pyspark-3.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"**Spark Configuration**","metadata":{"id":"lOHym6bSZDM4"}},{"cell_type":"code","source":"from pyspark import SparkContext,SparkConf\nconf=SparkConf().setAppName(\"test1\").setMaster(\"local[*]\") # The star mentions how many cores we want to use. * means use maximum possible cores.\nsc=SparkContext(conf=conf)","metadata":{"id":"5hK7Il0EYqcN","execution":{"iopub.status.busy":"2022-10-13T02:08:43.438230Z","iopub.execute_input":"2022-10-13T02:08:43.438733Z","iopub.status.idle":"2022-10-13T02:08:43.527789Z","shell.execute_reply.started":"2022-10-13T02:08:43.438622Z","shell.execute_reply":"2022-10-13T02:08:43.526054Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/362807903.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# The star mentions how many cores we want to use. * means use maximum possible cores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","output_type":"error"}]},{"cell_type":"markdown","source":"**Create RDD's and Basic Concepts**","metadata":{"id":"Fr5uri2JbjMX"}},{"cell_type":"code","source":"# Create random list\nimport random\n\nrandom_list=random.sample(range(0,30),10)\nprint(random_list)","metadata":{"id":"0wNQN76PW4j7","outputId":"3c048325-ddc2-47b5-87d3-c25e6f82c0db","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":"[11, 1, 3, 9, 17, 19, 20, 6, 8, 27]\n"}]},{"cell_type":"code","source":"# Create RDD\nrdd1=sc.parallelize(random_list,6)  # We create 4 partitions\nprint(rdd1.getNumPartitions())\nprint(rdd1.collect()) # This will collect data from executors and send it to driver. So, be careful while using actions which send output to driver.","metadata":{"id":"Ie0L2adFgeLk","outputId":"e4bdb52e-1156-4d7f-ca54-80f73ba81abf","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":"6\n\n[11, 1, 3, 9, 17, 19, 20, 6, 8, 27]\n"}]},{"cell_type":"code","source":"# View rdd in each partition\nrdd1.glom().collect()","metadata":{"id":"8kUHBD9QgrvT","outputId":"3929c4b5-5927-42e3-f922-19eb91057e59","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":["[[11], [1, 3], [9, 17], [19], [20, 6], [8, 27]]"]},"metadata":{}}]}]}